{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing gensim bigram implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG=\"french\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fdate=20170327\n",
    "fname=frwikinews-$fdate-cirrussearch-content.json.gz\n",
    "if [ ! -e  $fname ]\n",
    "then\n",
    "    wget \"https://dumps.wikimedia.org/other/cirrussearch/$fdate/$fname\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterator\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "FDATE = 20170327\n",
    "FNAME = \"frwikinews-%s-cirrussearch-content.json.gz\" % FDATE\n",
    "\n",
    "def iter_texts(fpath=FNAME):\n",
    "    with gzip.open(fpath, \"rt\") as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            if \"title\" in data:\n",
    "                yield data[\"title\"]\n",
    "                yield data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also prepare nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "fr_tokenizer = RegexpTokenizer('\\w[\\w-]*|\\d[\\d,]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(txt):\n",
    "    # lower case\n",
    "    txt = txt.lower()\n",
    "    return [fr_tokenizer.tokenize(sent) \n",
    "            for sent in sent_tokenize(txt, language=LANG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we put all data in ram, it's not so much\n",
    "corpus = []\n",
    "for txt in iter_texts():\n",
    "    corpus.extend(prepare(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 571431 words in 26255 sentences\n"
     ]
    }
   ],
   "source": [
    "# how many sentences and words ?\n",
    "words_count = sum(len(s) for s in corpus)\n",
    "print(\"Corpus has %d words in %d sentences\" % (words_count, len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases2 import Phrases as PhrasesCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'au aux avec ce ces dans de des du elle en et eux il je la le leur lui ma mais me même mes moi mon ne nos notre nous on ou par pas pour qu que qui sa se ses son sur ta te tes toi ton tu un une vos votre vous c d j l à m n s t y été étée étées étés étant étante étants étantes suis es est sommes êtes sont serai seras sera serons serez seront serais serait serions seriez seraient étais était étions étiez étaient fus fut fûmes fûtes furent sois soit soyons soyez soient fusse fusses fût fussions fussiez fussent ayant ayante ayantes ayants eu eue eues eus ai as avons avez ont aurai auras aura aurons aurez auront aurais aurait aurions auriez auraient avais avait avions aviez avaient eut eûmes eûtes eurent aie aies ait ayons ayez aient eusse eusses eût eussions eussiez eussent'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\" \".join(stopwords.words(LANG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = {\"create\": {}, \"use\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 707 ms per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o Phrases(corpus)\n",
    "times[\"create\"][\"std\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases.py:290: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.65 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(corpus)\n",
    "t = %timeit -o list(bigram[corpus])\n",
    "times[\"use\"][\"std\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram common terms, without using them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 703 ms per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o PhrasesCT(corpus)\n",
    "times[\"create\"][\"ct_none\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases2.py:324: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram2 = PhrasesCT(corpus)\n",
    "t = %timeit -o list(bigram2[corpus])\n",
    "times[\"use\"][\"ct_none\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram commons terms, effectively using them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 820 ms per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o PhrasesCT(corpus, common_terms=stopwords.words(LANG))\n",
    "times[\"create\"][\"ct\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases2.py:324: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.82 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram_ct = PhrasesCT(corpus, common_terms=stopwords.words(LANG))\n",
    "t = %%timeit -o list(bigram_ct[corpus])\n",
    "times[\"use\"][\"ct\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new bigram found thanks to common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1456.1700680272108, 'col du galibier'),\n",
       " (1486.5069444444443, 'feux d artifice'),\n",
       " (1585.6074074074074, 'déposé une plainte'),\n",
       " (1646.5923076923077, 'science et technologie'),\n",
       " (1832.4836811128944, 'traité de lisbonne'),\n",
       " (1899.914201183432, 'marine le pen'),\n",
       " (2123.581349206349, 'garde des sceaux'),\n",
       " (2363.0197944423294, 'vallée d aoste'),\n",
       " (2378.411111111111, 'outrage au drapeau'),\n",
       " (2503.590643274854, 'barrage de sivens'),\n",
       " (2623.247549019608, 'tribune de genève'),\n",
       " (2675.7125, 'dauphins de sète'),\n",
       " (2905.7511312217193, 'télégramme de brest'),\n",
       " (3057.957142857143, 'anneaux de saturne'),\n",
       " (3377.6252465483235, 'côte d ivoire'),\n",
       " (3822.4464285714284, 'cercle des nageurs'),\n",
       " (3822.4464285714284, 'pointée du doigt'),\n",
       " (4540.60303030303, 'défilé de robes'),\n",
       " (4896.728758169935, 'enrichissement d uranium'),\n",
       " (10614.396694214875, 'giscard d estaing')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_ngrams = sorted(list(set((g[1], g[0].decode(\"utf-8\"))\n",
    "                     for g in bigram_ct.export_phrases(corpus) \n",
    "                     if len(g[0].split()) > 2)))\n",
    "ct_ngrams[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tstd\tct_none\tct\n",
      "use\t1.650\t1.698\t1.816\n",
      "use%\t100%\t102%\t110%\n",
      "create\t0.707\t0.703\t0.820\n",
      "create%\t100%\t99%\t115%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tstd\\tct_none\\tct\")\n",
    "cols = [\"std\", \"ct_none\", \"ct\"]\n",
    "for k, v in times.items():\n",
    "    print(\"\\t\".join([k] + [\"%.3f\" % v[col] for col in cols]))\n",
    "    print(\"\\t\".join([k + \"%\"] + [\"%d%%\" % (v[col] / v[\"std\"] * 100) for col in cols]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
