{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing gensim bigram implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG=\"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fdate=20170327\n",
    "fname=enwikinews-$fdate-cirrussearch-content.json.gz\n",
    "if [ ! -e  $fname ]\n",
    "then\n",
    "    wget \"https://dumps.wikimedia.org/other/cirrussearch/$fdate/$fname\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterator\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "FDATE = 20170327\n",
    "FNAME = \"enwikinews-%s-cirrussearch-content.json.gz\" % FDATE\n",
    "\n",
    "def iter_texts(fpath=FNAME):\n",
    "    with gzip.open(fpath, \"rt\") as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            if \"title\" in data:\n",
    "                yield data[\"title\"]\n",
    "                yield data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also prepare nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "fr_tokenizer = RegexpTokenizer('\\w[\\w-]*|\\d[\\d,]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(txt):\n",
    "    # lower case\n",
    "    txt = txt.lower()\n",
    "    return [fr_tokenizer.tokenize(sent) \n",
    "            for sent in sent_tokenize(txt, language=LANG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we put all data in ram, it's not so much\n",
    "corpus = []\n",
    "for txt in iter_texts():\n",
    "    corpus.extend(prepare(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 1003521 words in 46159 sentences\n"
     ]
    }
   ],
   "source": [
    "# how many sentences and words ?\n",
    "words_count = sum(len(s) for s in corpus)\n",
    "print(\"Corpus has %d words in %d sentences\" % (words_count, len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases2 import Phrases as PhrasesCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\" \".join(stopwords.words(LANG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = {\"create\": {}, \"use\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.34 s per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o Phrases(corpus)\n",
    "times[\"create\"][\"std\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases.py:290: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.96 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(corpus)\n",
    "t = %timeit -o list(bigram[corpus])\n",
    "times[\"use\"][\"std\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram common terms, without using them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.34 s per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o PhrasesCT(corpus)\n",
    "times[\"create\"][\"ct_none\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases2.py:324: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 3.01 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram2 = PhrasesCT(corpus)\n",
    "t = %timeit -o list(bigram2[corpus])\n",
    "times[\"use\"][\"ct_none\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram commons terms, effectively using them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.52 s per loop\n"
     ]
    }
   ],
   "source": [
    "t = %timeit -o PhrasesCT(corpus, common_terms=stopwords.words(LANG))\n",
    "times[\"create\"][\"ct\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gensim/gensim/models/phrases2.py:324: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 3.47 s per loop\n"
     ]
    }
   ],
   "source": [
    "bigram_ct = PhrasesCT(corpus, common_terms=stopwords.words(LANG))\n",
    "t = %%timeit -o list(bigram_ct[corpus])\n",
    "times[\"use\"][\"ct\"] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new bigram found thanks to common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1300.0463768115942, 'chamber of deputies'),\n",
       " (1632.9466019417475, 'globe and mail'),\n",
       " (1648.9558823529412, 'chosen to involve'),\n",
       " (1661.1703703703704, 'thoughts and prayers'),\n",
       " (1820.2759740259742, 'thank you for listening'),\n",
       " (2034.0861678004535, 'hall of fame'),\n",
       " (2038.7090909090912, 'saturn s rings'),\n",
       " (2046.7992063492065, 'colleges and universities'),\n",
       " (2777.188854489164, 'burst into flames'),\n",
       " (2920.026041666667, 'divide by zero'),\n",
       " (2950.7631578947367, 'emerges from recession'),\n",
       " (3136.4755244755243, 'serbia and montenegro'),\n",
       " (3219.014354066985, 'compilation of brief'),\n",
       " (3297.9117647058824, 'accessed on 2006-12-10'),\n",
       " (3297.9117647058824, 'accessed on 2006-12-11'),\n",
       " (4548.843813387424, 'parks and recreation'),\n",
       " (4651.277037037037, 'monsters and critics'),\n",
       " (5333.703783783783, 'disasters and accidents'),\n",
       " (5862.954248366013, 'skull and bones'),\n",
       " (7081.831578947369, 'click on the donate')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_ngrams = sorted(list(set((g[1], g[0].decode(\"utf-8\"))\n",
    "                     for g in bigram_ct.export_phrases(corpus) \n",
    "                     if len(g[0].split()) > 2)))\n",
    "ct_ngrams[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tstd\tct_none\tct\n",
      "create\t1.336\t1.336\t1.516\n",
      "create%\t100%\t99%\t113%\n",
      "use\t2.958\t3.015\t3.466\n",
      "use%\t100%\t101%\t117%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tstd\\tct_none\\tct\")\n",
    "cols = [\"std\", \"ct_none\", \"ct\"]\n",
    "for k, v in times.items():\n",
    "    print(\"\\t\".join([k] + [\"%.3f\" % v[col] for col in cols]))\n",
    "    print(\"\\t\".join([k + \"%\"] + [\"%d%%\" % (v[col] / v[\"std\"] * 100) for col in cols]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
