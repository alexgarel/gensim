{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating common terms usage using Wikinews in english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting data\n",
    "\n",
    "We get the cirrussearch dump of wikinews (a dump meant for elastic-search indexation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG=\"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fdate=20170327\n",
    "fname=enwikinews-$fdate-cirrussearch-content.json.gz\n",
    "if [ ! -e  $fname ]\n",
    "then\n",
    "    wget \"https://dumps.wikimedia.org/other/cirrussearch/$fdate/$fname\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterator\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "FDATE = 20170327\n",
    "FNAME = \"enwikinews-%s-cirrussearch-content.json.gz\" % FDATE\n",
    "\n",
    "def iter_texts(fpath=FNAME):\n",
    "    with gzip.open(fpath, \"rt\") as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            if \"title\" in data:\n",
    "                yield data[\"title\"]\n",
    "                yield data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also prepare nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "\n",
    "we arrange the corpus as required by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a custom tokenizer\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w[\\w-]*|\\d[\\d,]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare a text\n",
    "def prepare(txt):\n",
    "    # lower case\n",
    "    txt = txt.lower()\n",
    "    return [tokenizer.tokenize(sent) \n",
    "            for sent in sent_tokenize(txt, language=LANG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we put all data in ram, it's not so much\n",
    "corpus = []\n",
    "for txt in iter_texts():\n",
    "    corpus.extend(prepare(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 1003521 words in 46159 sentences\n"
     ]
    }
   ],
   "source": [
    "# how many sentences and words ?\n",
    "words_count = sum(len(s) for s in corpus)\n",
    "print(\"Corpus has %d words in %d sentences\" % (words_count, len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing bigram with and without common terms\n",
    "\n",
    "The `Phrases` model gives us the possiblity of handling common terms, that is words that appears much time in a text and are there only to link objects between them.\n",
    "While you could remove them, you may information, for *\"the president is in america\"* is not the same as *\"the president of america\"*\n",
    "\n",
    "The common_terms parameter Phrases can help you deal with them in a smarter way, keeping them around but avoiding them to crush frequency statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which are the stop words we will use\n",
    "from nltk.corpus import stopwords\n",
    "\" \".join(stopwords.words(LANG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 20 ms, total: 1.36 s\n",
      "Wall time: 1.36 s\n",
      "CPU times: user 1.6 s, sys: 24 ms, total: 1.62 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "# bigram std\n",
    "%time bigram = Phrases(corpus)\n",
    "# bigram with common terms\n",
    "%time bigram_ct = Phrases(corpus, common_terms=stopwords.words(LANG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram with common terms inside\n",
    "\n",
    "What are (some of) the bigram founds thanks to common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564 grams with common terms found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7888.587500000001, 'christ of latter-day'),\n",
       " (8249.503267973856, 'skull and bones'),\n",
       " (8452.058035714284, 'preserved in amber'),\n",
       " (8624.480689245396, 'aisyah and doan'),\n",
       " (8664.810068649886, 'funded by your generous'),\n",
       " (9015.528571428571, 'restored as burkina'),\n",
       " (9964.531578947368, 'click on the donate'),\n",
       " (10178.822580645161, 'qatar of intervening'),\n",
       " (10380.724721819062, 'sinks in suva'),\n",
       " (11462.050213675215, 'lahm to hang'),\n",
       " (11485.911021233569, 'istanbul s ataturk'),\n",
       " (11686.796296296296, 'derails in tabasco'),\n",
       " (12749.232323232325, 'poet of apostasy'),\n",
       " (13499.187165775402, 'creator of kinder'),\n",
       " (14791.1015625, 'consulate in irbil'),\n",
       " (17133.58371040724, 'newsworthy and entertaining'),\n",
       " (17460.90513833992, 'screened on billboard'),\n",
       " (22273.65882352941, 'santos over nepotism'),\n",
       " (22896.580498866213, 'hotness of bhut'),\n",
       " (28048.311111111107, 'republic-turkey and croatia-spain')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grams that have more than 2 terms, are those with common terms\n",
    "ct_ngrams = set((g[1], g[0].decode(\"utf-8\"))\n",
    "                     for g in bigram_ct.export_phrases(corpus) \n",
    "                     if len(g[0].split()) > 2)\n",
    "ct_ngrams = sorted(list(ct_ngrams))\n",
    "print(len(ct_ngrams), \"grams with common terms found\")\n",
    "# highest scores\n",
    "ct_ngrams[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common terms influence parameters\n",
    "\n",
    "While using common_terms, you may have different results also on normal bigrams, \n",
    "1. there are no more bigram composed of a word + a common terms\n",
    "2. as the vocabulary size may increase when using common_terms, you may gain some more bigrams (which could be removed by adjusting the threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242  2-grams not found\n",
      "1404  more 2-grams found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['swimming championships',\n",
       "  'film 28',\n",
       "  'malaria drug',\n",
       "  'pakistan collaborate',\n",
       "  'russians continue',\n",
       "  'spacecraft crashes',\n",
       "  'and croatia-spain',\n",
       "  'most proud',\n",
       "  'for anzac',\n",
       "  'in napier',\n",
       "  'for airasia',\n",
       "  'burst into',\n",
       "  'seek asylum',\n",
       "  'm eastern',\n",
       "  'for up-to-date',\n",
       "  'indonesia collaborate',\n",
       "  'in hotness',\n",
       "  'as burkina',\n",
       "  'missile from',\n",
       "  'does toronto'],\n",
       " ['3 000',\n",
       "  '25 000',\n",
       "  'obama presidential',\n",
       "  'jm it',\n",
       "  'april 5',\n",
       "  'china russia',\n",
       "  'has taken',\n",
       "  'arrest karachi',\n",
       "  'continue their',\n",
       "  'buffalo fire',\n",
       "  'january 13',\n",
       "  'would take',\n",
       "  'we must',\n",
       "  'report released',\n",
       "  'opposition party',\n",
       "  'banned from',\n",
       "  'second train',\n",
       "  'com november',\n",
       "  'astronaut eugene',\n",
       "  've seen'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some differences on normal terms\n",
    "ct_normal_grams = set(g[0].decode(\"utf-8\")\n",
    "                     for g in bigram_ct.export_phrases(corpus)\n",
    "                     if len(g[0].split()) <= 2)\n",
    "no_ct_normal_grams = set(g[0].decode(\"utf-8\") for g in bigram.export_phrases(corpus))\n",
    "not_found = list(no_ct_normal_grams - ct_normal_grams)\n",
    "print(len(not_found), \" 2-grams not found\")\n",
    "new_found = list(ct_normal_grams - no_ct_normal_grams)\n",
    "print(len(new_found), \" more 2-grams found\")\n",
    "not_found[-20:], new_found[-20:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
